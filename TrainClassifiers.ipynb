{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifiers\n",
    "### Procedures:\n",
    "- Add your collected dataset to variable [your_org]_dataset\n",
    "- Split your dataset by article in to tuples of (article, 'news_org') and append to docs\n",
    "- Tokenize words, then append them to all_words\n",
    "- In the Visualization cell, add your news org to the classes list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random \n",
    "import numpy\n",
    "import scipy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "#############################################\n",
    "# ADD YOUR DATASET HERE\n",
    "cnn_dataset = open('./Datasets/cnn_dataset', 'r').read()\n",
    "bbc_dataset = open('./Datasets/bbc_dataset', 'r').read()\n",
    "nyt_dataset = open('./Datasets/bbc_dataset', 'r').read()\n",
    "\n",
    "#############################################\n",
    "\n",
    "#def clean_dataset(dataset):\n",
    "#    cleanset = []\n",
    "#    for word in dataset:\n",
    "#        if word not in stopwords:\n",
    "#            cleanset.append(word)\n",
    "#    return cleanset\n",
    "\n",
    "#cnn_cleanset = clean_dataset(cnn_dataset)\n",
    "#bbc_cleanset = clean_dataset(bbc_dataset)\n",
    "\n",
    "docs = []\n",
    "all_words = []\n",
    "\n",
    "#################################################\n",
    "# APPEND TUPLES TO DOC\n",
    "for r in cnn_dataset.split('\\n'):\n",
    "    docs.append((r, 'cnn'))\n",
    "    \n",
    "for r in bbc_dataset.split('\\n'):\n",
    "    docs.append((r, 'bbc'))\n",
    "    \n",
    "for r in nyt_dataset.split('\\n'):\n",
    "    docs.append((r, 'nyt'))\n",
    "\n",
    "\n",
    "# TOKENIZE    \n",
    "cnn_words = nltk.word_tokenize(cnn_dataset)\n",
    "bbc_words = nltk.word_tokenize(bbc_dataset)\n",
    "nyt_words = nltk.word_tokenize(nyt_dataset)\n",
    "\n",
    "# APPEND LOWERCASE WORDS TO ALL WORDS\n",
    "for w in cnn_words:\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "for w in bbc_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "for w in nyt_words:\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "###################################################    \n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = list(all_words.keys())\n",
    "\n",
    "def find_features(doc):\n",
    "    words = nltk.word_tokenize(doc)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(find_features(art), org) for (art, org) in docs]\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "# CHANGE SIZE OF TRAINING AND TESTING SETS HOW YOU SEE FIT\n",
    "train_set, test_set = featuresets[:300], featuresets[300:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"NB Acc:\", nltk.classify.accuracy(classifier, test_set) * 100)\n",
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SGD_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGD_classifier.train(train_set)\n",
    "print(\"SGD Acc:\", nltk.classify.accuracy(SGD_classifier, test_set) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "ref = [org for (art, org) in test_set]\n",
    "test = [SGD_classifier.classify(art) for (art, org) in test_set]\n",
    "\n",
    "cm = nltk.ConfusionMatrix(ref, test)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))\n",
    "\n",
    "sklcm = confusion_matrix(ref, test)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "def plot_cm(cm, classes, title, cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    \n",
    "plt.figure()\n",
    "plot_cm(sklcm, classes=['bbc', 'cnn', 'nyt'], title='SGD Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Classifiers\n",
    "import pickle\n",
    "\n",
    "pickle.dump(classifier, open('./Classifiers/classifier.pickle', 'wb'))\n",
    "pickle.dump(SGD_classifier, open('./Classifiers/sgd_classifier.pickle', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
