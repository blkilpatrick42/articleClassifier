{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifiers\n",
    "### Procedures:\n",
    "- Add your collected dataset to variable [your_org]_dataset\n",
    "- Split your dataset by article in to tuples of (article, 'news_org') and append to docs\n",
    "- Tokenize words, then append them to all_words\n",
    "- In the Visualization cell, add your news org to the classes list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of featuresets: 1021\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random \n",
    "import numpy\n",
    "import scipy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "#############################################\n",
    "# ADD YOUR DATASET HERE\n",
    "cnn_dataset = open('./Datasets/clean_cnn_dataset', 'r').read()\n",
    "bbc_dataset = open('./Datasets/clean_bbc_dataset', 'r').read()\n",
    "nyt_dataset = open('./Datasets/clean_nyt_dataset', 'r').read()\n",
    "fox_dataset = open('./Datasets/clean_fox_dataset', 'r').read()\n",
    "#alj_dataset = open('./Datasets/aljDataset', 'r').read()\n",
    "\n",
    "#############################################\n",
    "\n",
    "#def clean_dataset(dataset):\n",
    "#    cleanset = []\n",
    "#    for word in dataset:\n",
    "#        if word not in stopwords:\n",
    "#            cleanset.append(word)\n",
    "#    return cleanset\n",
    "\n",
    "#cnn_cleanset = clean_dataset(cnn_dataset)\n",
    "#bbc_cleanset = clean_dataset(bbc_dataset)\n",
    "\n",
    "docs = []\n",
    "all_words = []\n",
    "\n",
    "#################################################\n",
    "# APPEND TUPLES TO DOC\n",
    "for r in cnn_dataset.split('\\n'):\n",
    "    docs.append((r, 'cnn'))\n",
    "    \n",
    "for r in bbc_dataset.split('\\n'):\n",
    "    docs.append((r, 'bbc'))\n",
    "    \n",
    "for r in nyt_dataset.split('\\n'):\n",
    "    docs.append((r, 'nyt'))\n",
    "    \n",
    "for r in fox_dataset.split('\\n'):\n",
    "    docs.append((r, 'fox'))\n",
    "    \n",
    "#for r in alj_dataset.split('\\n'):\n",
    " #   docs.append((r, 'alj'))\n",
    "    \n",
    "\n",
    "\n",
    "# TOKENIZE    \n",
    "cnn_words = nltk.word_tokenize(cnn_dataset)\n",
    "bbc_words = nltk.word_tokenize(bbc_dataset)\n",
    "nyt_words = nltk.word_tokenize(nyt_dataset)\n",
    "fox_words = nltk.word_tokenize(fox_dataset)\n",
    "#alj_words = nltk.word_tokenize(alj_dataset)\n",
    "\n",
    "# APPEND LOWERCASE WORDS TO ALL WORDS\n",
    "for w in cnn_words:\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "for w in bbc_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "for w in nyt_words:\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "for w in fox_words:\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "#for w in alj_words:\n",
    "#    all_words.append(w.lower())\n",
    "    \n",
    "###################################################    \n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = list(all_words.keys())\n",
    "\n",
    "def find_features(doc):\n",
    "    words = nltk.word_tokenize(doc)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(find_features(art), org) for (art, org) in docs]\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "# CHANGE SIZE OF TRAINING AND TESTING SETS HOW YOU SEE FIT\n",
    "print('Length of featuresets:', len(featuresets))\n",
    "train_set, test_set = featuresets[200:], featuresets[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Acc: 65.5\n",
      "Most Informative Features\n",
      "                     cnn = True              cnn : bbc    =    128.8 : 1.0\n",
      "                       — = True              nyt : bbc    =    123.8 : 1.0\n",
      "                       t = True              nyt : bbc    =    108.6 : 1.0\n",
      "                 unfolds = True              cnn : nyt    =     88.2 : 1.0\n",
      "                     mr. = True              nyt : bbc    =     86.5 : 1.0\n",
      "                       ” = True              nyt : bbc    =     85.7 : 1.0\n",
      "                       . = False             fox : nyt    =     83.0 : 1.0\n",
      "                      et = True              cnn : fox    =     80.3 : 1.0\n",
      "                       - = True              bbc : nyt    =     76.1 : 1.0\n",
      "                 updated = True              cnn : fox    =     74.5 : 1.0\n",
      "                      re = True              nyt : bbc    =     64.9 : 1.0\n",
      "                       “ = True              nyt : bbc    =     61.2 : 1.0\n",
      "                     don = True              nyt : bbc    =     59.9 : 1.0\n",
      "                      `` = True              bbc : nyt    =     58.5 : 1.0\n",
      "                reserved = True              fox : nyt    =     55.2 : 1.0\n",
      "                       , = False             fox : nyt    =     54.2 : 1.0\n",
      "                    chat = True              cnn : nyt    =     53.8 : 1.0\n",
      "                     bio = True              cnn : nyt    =     53.8 : 1.0\n",
      "               happening = True              cnn : fox    =     53.4 : 1.0\n",
      "               rewritten = True              fox : bbc    =     52.5 : 1.0\n",
      "                     bbc = True              bbc : fox    =     52.0 : 1.0\n",
      "                    past = True              nyt : fox    =     49.4 : 1.0\n",
      "                     ms. = True              nyt : fox    =     49.4 : 1.0\n",
      "                 percent = True              nyt : bbc    =     45.9 : 1.0\n",
      "                      pm = True              cnn : nyt    =     45.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"NB Acc:\", nltk.classify.accuracy(classifier, test_set) * 100)\n",
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Acc: 95.5\n"
     ]
    }
   ],
   "source": [
    "SGD_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGD_classifier.train(train_set)\n",
    "print(\"SGD Acc:\", nltk.classify.accuracy(SGD_classifier, test_set) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |      b      n      f      c |\n",
      "    |      b      y      o      n |\n",
      "    |      c      t      x      n |\n",
      "----+-----------------------------+\n",
      "bbc | <32.5%>     .   0.5%   0.5% |\n",
      "nyt |      . <28.0%>  1.0%      . |\n",
      "fox |      .   2.0% <23.0%>  0.5% |\n",
      "cnn |      .      .      . <12.0%>|\n",
      "----+-----------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nsklcm = confusion_matrix(ref, test)\\nprint(sklcm)\\nnp.set_printoptions(precision=2)\\n\\ndef plot_cm(cm, classes, title, cmap=plt.cm.Blues):\\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\\n    plt.title(title)\\n    plt.colorbar()\\n    tick_marks = np.arange(len(classes))\\n    plt.xticks(tick_marks, classes, rotation=45)\\n    plt.yticks(tick_marks, classes)\\n    plt.tight_layout()\\n    plt.ylabel('True Class')\\n    plt.xlabel('Predicted Class')\\n    \\nplt.figure()\\nplot_cm(sklcm, classes=['bbc', 'nyt', 'cnn'], title='SGD Classifier')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "#print('Size of test_set', len(test_set))\n",
    "#for test in test_set:\n",
    "#    if(test[1] == 'fox'):\n",
    "#        print(test[0])\n",
    "        \n",
    "ref = [org for (art, org) in test_set]\n",
    "test = [SGD_classifier.classify(art) for (art, org) in test_set]\n",
    "\n",
    "cm = nltk.ConfusionMatrix(ref, test)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))\n",
    "\n",
    "\n",
    "'''\n",
    "sklcm = confusion_matrix(ref, test)\n",
    "print(sklcm)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "def plot_cm(cm, classes, title, cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    \n",
    "plt.figure()\n",
    "plot_cm(sklcm, classes=['bbc', 'nyt', 'cnn'], title='SGD Classifier')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Classifiers\n",
    "import pickle\n",
    "\n",
    "pickle.dump(classifier, open('./Classifiers/classifier.pickle', 'wb'))\n",
    "pickle.dump(SGD_classifier, open('./Classifiers/sgd_classifier.pickle', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
